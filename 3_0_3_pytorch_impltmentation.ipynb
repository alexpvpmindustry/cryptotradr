{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas_ta as ta\n",
    "from multiprocessing.pool import Pool\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol     Security             GICS Sector               GICS Sub-Industry  \\\n",
      "0    MMM           3M             Industrials        Industrial Conglomerates   \n",
      "1    AOS  A. O. Smith             Industrials               Building Products   \n",
      "2    ABT       Abbott             Health Care           Health Care Equipment   \n",
      "3   ABBV       AbbVie             Health Care                   Biotechnology   \n",
      "4    ACN    Accenture  Information Technology  IT Consulting & Other Services   \n",
      "\n",
      "     Headquarters Location  Date added      CIK      Founded  \n",
      "0    Saint Paul, Minnesota  1957-03-04    66740         1902  \n",
      "1     Milwaukee, Wisconsin  2017-07-26    91142         1916  \n",
      "2  North Chicago, Illinois  1957-03-04     1800         1888  \n",
      "3  North Chicago, Illinois  2012-12-31  1551152  2013 (1888)  \n",
      "4          Dublin, Ireland  2011-07-06  1467373         1989  \n"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_html(\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "print(tickers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"hist_data_2018-1-1_2023-7-12.pkl\",\"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data[\"Open\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABNB\n",
      "BF.B\n",
      "BRK.B\n",
      "CARR\n",
      "CDAY\n",
      "CEG\n",
      "CTVA\n",
      "DOW\n",
      "FOX\n",
      "FOXA\n",
      "GEHC\n",
      "KVUE\n",
      "MRNA\n",
      "OTIS\n",
      "UBER\n",
      "VLTO\n"
     ]
    }
   ],
   "source": [
    "coli_with_na = np.where(np.sum(pd.isna(data[\"Close\"]))>0)[0]\n",
    "for coli in coli_with_na:\n",
    "    print(columns[coli])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opens= data[\"Open\"].drop(columns=[columns[coli] for coli in coli_with_na])\n",
    "closes = data[\"Close\"].drop(columns=[columns[coli] for coli in coli_with_na])\n",
    "# Open\tHigh\tLow\tClose\tVolume\n",
    "highs = data[\"High\"].drop(columns=[columns[coli] for coli in coli_with_na])\n",
    "lows = data[\"Low\"].drop(columns=[columns[coli] for coli in coli_with_na])\n",
    "volumes = data[\"Volume\"].drop(columns=[columns[coli] for coli in coli_with_na])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1389, 487), (1389, 487), (1389, 487), (1389, 487), (1389, 487))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opens.shape,closes.shape,highs.shape,lows.shape,volumes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker=\"TSLA\"\n",
    "training_X=[]\n",
    "training_Y=[]\n",
    "correlation_Y = []\n",
    "temp_df_full = pd.DataFrame(data={\"Open\":opens[ticker],\n",
    "                         \"High\":highs[ticker],\n",
    "                         \"Low\":lows[ticker],\n",
    "                         \"Close\":closes[ticker],\n",
    "                         \"Volume\":volumes[ticker],\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(df,w_min,w_max):\n",
    "    return (df-w_min)/(w_max-w_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"hist_data_2018-1-1_2023-7-12_3_0_2.pkl\",\"rb\") as f:\n",
    "    [training_X,training_Y,correlation_Y] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66232, 14), (66232,), (66232,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_X.shape,training_Y.shape,correlation_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 52931, 1: 7168, 2: 6133})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.02\n",
    "train_data_y_discrete = np.asarray(list(map(lambda x: 2 if (x>threshold) else (1 if x<-threshold else 0),training_Y)))\n",
    "Counter(train_data_y_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = np.hstack([np.where(train_data_y_discrete!=0)[0],np.where(train_data_y_discrete==0)[0][::8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 7168, 2: 6133, 0: 6617})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_data_y_discrete[sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(training_X[sel], train_data_y_discrete[sel], test_size=0.3,random_state=109,shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_X, train_data_y_discrete, test_size=0.3,random_state=109,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46362, 14), (19870, 14), (46362,), (19870,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPClassifierPyTorch(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=3):\n",
    "        super(MLPClassifierPyTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 150)\n",
    "        self.fc2 = nn.Linear(150, 150)\n",
    "        self.fc3 = nn.Linear(150, 50)\n",
    "        self.fc4 = nn.Linear(50, 10)\n",
    "        self.fc5 = nn.Linear(10, 50)\n",
    "        self.fc6 = nn.Linear(50, 150)\n",
    "        self.fc7 = nn.Linear(150, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = self.fc7(x)  # No activation here, CrossEntropyLoss will apply softmax\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66232"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_y_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X' and 'y' are your features and labels with shapes (1000, 14) and (1000,)\n",
    "# Convert them to PyTorch tensors\n",
    "#X = torch.tensor(training_X[sel], dtype=torch.float32)\n",
    "#y = torch.tensor(train_data_y_discrete[sel], dtype=torch.long)\n",
    "X = torch.tensor(training_X, dtype=torch.float32)\n",
    "y = torch.tensor(train_data_y_discrete, dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15,shuffle=True)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 5000#128*4\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "input_size = 14\n",
    "model = MLPClassifierPyTorch(input_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0.90, 1  0.66, 2  0.63, 3  0.64, 4  0.63, 5  0.62, 6  0.63, 7  0.64, 8  0.64, 9  0.63, 10  0.64, 11  0.66, 12  0.64, 13  0.67, 14  0.65, 15  0.67, 16  0.62, 17  0.62, 18  0.63, 19  0.65, 20  0.65, 21  0.64, 22  0.67, 23  0.67, 24  0.66, 25  0.65, 26  0.62, 27  0.64, 28  0.64, 29  0.62, 30  0.65, 31  0.64, 32  0.63, 33  0.65, 34  0.63, 35  0.64, 36  0.64, 37  0.65, 38  0.63, 39  0.64, 40  0.64, 41  0.61, 42  0.62, 43  0.64, 44  0.66, 45  0.63, 46  0.63, 47  0.63, 48  0.62, 49  0.63, 50  0.62, 51  0.63, 52  0.61, 53  0.65, 54  0.62, 55  0.62, 56  0.65, 57  0.62, 58  0.65, 59  0.64, 60  0.60, 61  0.61, 62  0.63, 63  0.61, 64  0.62, 65  0.63, 66  0.60, 67  0.66, 68  0.61, 69  0.62, 70  0.64, 71  0.62, 72  0.62, 73  0.63, 74  0.63, 75  0.63, 76  0.65, 77  0.65, 78  0.65, 79  0.61, 80  0.65, 81  0.60, 82  0.64, 83  0.62, 84  0.64, 85  0.62, 86  0.64, 87  0.63, 88  0.62, 89  0.64, 90  0.66, 91  0.64, 92  0.63, 93  0.62, 94  0.64, 95  0.63, 96  0.65, 97  0.63, 98  0.60, 99  0.67, 100  0.64, 101  0.61, 102  0.64, 103  0.65, 104  0.64, 105  0.65, 106  0.64, 107  0.60, 108  0.61, 109  0.63, 110  0.63, 111  0.64, 112  0.61, 113  0.65, 114  0.64, 115  0.65, 116  0.61, 117  0.63, 118  0.63, 119  0.65, 120  0.64, 121  0.61, 122  0.60, 123  0.66, 124  0.62, 125  0.64, 126  0.61, 127  0.65, 128  0.63, 129  0.62, 130  0.63, 131  0.61, 132  0.64, 133  0.62, 134  0.62, 135  0.62, 136  0.63, 137  0.64, 138  0.65, 139  0.64, 140  0.64, 141  0.62, 142  0.64, 143  0.63, 144  0.63, 145  0.61, 146  0.63, 147  0.62, 148  0.59, 149  0.63, 150  0.62, 151  0.64, 152  0.64, 153  0.62, 154  0.63, 155  0.61, 156  0.66, 157  0.63, 158  0.63, 159  0.64, 160  0.62, 161  0.64, 162  0.65, 163  0.63, 164  0.63, 165  0.64, 166  0.63, 167  0.62, 168  0.62, 169  0.64, 170  0.61, 171  0.63, 172  0.63, 173  0.65, 174  0.67, 175  0.62, 176  0.59, 177  0.63, 178  0.62, 179  0.61, 180  0.63, 181  0.62, 182  0.67, 183  0.64, 184  0.62, 185  0.63, 186  0.64, 187  0.62, 188  0.62, 189  0.61, 190  0.60, 191  0.63, 192  0.62, 193  0.64, 194  0.64, 195  0.62, 196  0.62, 197  0.62, 198  0.62, 199  0.63, 200  0.63, 201  0.64, 202  0.61, 203  0.60, 204  0.63, 205  0.62, 206  0.64, 207  0.64, 208  0.64, 209  0.63, 210  0.61, 211  0.64, 212  0.64, 213  0.64, 214  0.67, 215  0.63, 216  0.64, 217  0.63, 218  0.63, 219  0.66, 220  0.59, 221  0.62, 222  0.63, 223  0.64, 224  0.60, 225  0.62, 226  0.66, 227  0.63, 228  0.63, 229  0.63, 230  0.64, 231  0.65, 232  0.63, 233  0.63, 234  0.63, 235  0.62, 236  0.65, 237  0.62, 238  0.61, 239  0.63, 240  0.63, 241  0.67, 242  0.64, 243  0.65, 244  0.64, 245  0.61, 246  0.62, 247  0.65, 248  0.62, 249  0.62, CPU times: total: 3min 33s\n",
      "Wall time: 6min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training loop\n",
    "loss_history = []\n",
    "for epoch in range(250):\n",
    "    print(epoch,end=\" \")\n",
    "    avg_loss = []\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        avg_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_history.append(np.mean(avg_loss))\n",
    "    print(f\" {loss.item():.2f},\",end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Precision: 0.80, Recall: 1.00\n",
      "Class 1 - Precision: 0.00, Recall: 0.00\n",
      "Class 2 - Precision: 0.00, Recall: 0.00\n",
      "CPU times: total: 484 ms\n",
      "Wall time: 613 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aatan\\Documents\\Github\\cryptotradr_py38\\cryptotradr\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate precision and recall for each class\n",
    "precision = precision_score(all_labels, all_predictions, average=None)\n",
    "recall = recall_score(all_labels, all_predictions, average=None)\n",
    "\n",
    "# Print precision and recall for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f'Class {i} - Precision: {precision[i]:.2f}, Recall: {recall[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = torch.tensor(training_X, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(train_data_y_discrete, dtype=torch.long).to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 2000\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the selected device\n",
    "input_size = 14\n",
    "model = MLPClassifierPyTorch(input_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.0, 30.0]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0.68, 1  0.66, 2  0.64, 3  0.63, 4  0.65, 5  0.63, 6  0.58, 7  0.69, 8  0.67, 9  0.62, 10  0.60, 11  0.62, 12  0.64, 13  0.69, 14  0.69, 15  0.60, 16  0.67, 17  0.60, 18  0.68, 19  0.67, 20  0.70, 21  0.61, 22  0.66, 23  0.64, 24  0.66, 25  0.61, 26  0.62, 27  0.61, 28  0.58, 29  0.61, 30  0.60, 31  0.66, 32  0.68, 33  0.69, 34  0.68, 35  0.62, 36  0.61, 37  0.71, 38  0.63, 39  0.62, 40  0.68, 41  0.63, 42  0.61, 43  0.61, 44  0.69, 45  0.69, 46  0.67, 47  0.65, 48  0.61, 49  0.68, 50  0.62, 51  0.56, 52  0.71, 53  0.64, 54  0.63, 55  0.61, 56  0.64, 57  0.61, 58  0.63, 59  0.67, 60  0.61, 61  0.61, 62  0.62, 63  0.63, 64  0.65, 65  0.61, 66  0.68, 67  0.64, 68  0.59, 69  0.63, 70  0.62, 71  0.67, 72  0.60, 73  0.67, 74  0.65, 75  0.65, 76  0.66, 77  0.63, 78  0.60, 79  0.61, 80  0.63, 81  0.66, 82  0.65, 83  0.65, 84  0.65, 85  0.63, 86  0.66, 87  0.60, 88  0.62, 89  0.63, 90  0.64, 91  0.65, 92  0.61, 93  0.64, 94  0.65, 95  0.60, 96  0.64, 97  0.59, 98  0.68, 99  0.62, 100  0.57, 101  0.68, 102  0.61, 103  0.61, 104  0.62, 105  0.68, 106  0.62, 107  0.64, 108  0.58, 109  0.64, 110  0.66, 111  0.62, 112  0.65, 113  0.64, 114  0.66, 115  0.61, 116  0.62, 117  0.62, 118  0.69, 119  0.68, 120  0.63, 121  0.62, 122  0.66, 123  0.62, 124  0.65, 125  0.67, 126  0.61, 127  0.60, 128  0.59, 129  0.61, 130  0.62, 131  0.58, 132  0.64, 133  0.65, 134  0.61, 135  0.68, 136  0.61, 137  0.64, 138  0.62, 139  0.62, 140  0.70, 141  0.58, 142  0.61, 143  0.67, 144  0.62, 145  0.60, 146  0.59, 147  0.65, 148  0.63, 149  0.61, 150  0.69, 151  0.66, 152  0.62, 153  0.65, 154  0.65, 155  0.63, 156  0.59, 157  0.66, 158  0.59, 159  0.62, 160  0.64, 161  0.62, 162  0.64, 163  0.62, 164  0.56, 165  0.60, 166  0.66, 167  0.62, 168  0.60, 169  0.63, 170  0.64, 171  0.60, 172  0.60, 173  0.65, 174  0.65, 175  0.67, 176  0.62, 177  0.66, 178  0.59, 179  0.59, 180  0.65, 181  0.60, 182  0.65, 183  0.65, 184  0.61, 185  0.63, 186  0.65, 187  0.66, 188  0.63, 189  0.61, 190  0.60, 191  0.64, 192  0.61, 193  0.61, 194  0.63, 195  0.64, 196  0.62, 197  0.63, 198  0.64, 199  0.65, 200  0.61, 201  0.60, 202  0.63, 203  0.63, 204  0.63, 205  0.71, 206  0.65, 207  0.64, 208  0.61, 209  0.65, 210  0.67, 211  0.61, 212  0.62, 213  0.61, 214  0.55, 215  0.59, 216  0.62, 217  0.63, 218  0.63, 219  0.60, 220  0.60, 221  0.66, 222  0.64, 223  0.62, 224  0.61, 225  0.61, 226  0.58, 227  0.63, 228  0.61, 229  0.66, 230  0.69, 231  0.64, 232  0.62, 233  0.69, 234  0.61, 235  0.61, 236  0.63, 237  0.62, 238  0.69, 239  0.63, 240  0.62, 241  0.66, 242  0.63, 243  0.67, 244  0.64, 245  0.64, 246  0.66, 247  0.68, 248  0.59, 249  0.63, CPU times: total: 3min 8s\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training loop\n",
    "loss_history = []\n",
    "for epoch in range(250):\n",
    "    print(epoch, end=\" \")\n",
    "    avg_loss = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        avg_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_history.append(np.mean(avg_loss))\n",
    "    print(f\" {loss.item():.2f},\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Precision: 0.88, Recall: 0.00\n",
      "Class 1 - Precision: 0.00, Recall: 0.00\n",
      "Class 2 - Precision: 0.09, Recall: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aatan\\Documents\\Github\\cryptotradr_py38\\cryptotradr\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate precision and recall for each class\n",
    "precision = precision_score(all_labels, all_predictions, average=None)\n",
    "recall = recall_score(all_labels, all_predictions, average=None)\n",
    "\n",
    "# Print precision and recall for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f'Class {i} - Precision: {precision[i]:.2f}, Recall: {recall[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha  # Interpolation factor for combining losses\n",
    "        self.beta = beta  # Balancing factor between false positives and false negatives for class 2\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Standard CrossEntropyLoss\n",
    "        ce_loss = self.cross_entropy_loss(outputs, targets)\n",
    "\n",
    "        # Custom loss focusing on label 2 (both false positives and false negatives)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        label_2_probs = probabilities[:, 2]\n",
    "\n",
    "        # Penalize false positives for label 2\n",
    "        non_label_2_mask = (targets != 2)\n",
    "        false_positives = label_2_probs * non_label_2_mask.float()\n",
    "\n",
    "        # Penalize false negatives for label 2\n",
    "        label_2_mask = (targets == 2)\n",
    "        false_negatives = (1 - label_2_probs) * label_2_mask.float()\n",
    "\n",
    "        # Combined custom loss for label 2\n",
    "        custom_loss = self.beta * false_positives.sum() + (1 - self.beta) * false_negatives.sum()\n",
    "\n",
    "        # Combined loss\n",
    "        combined_loss = (1 - self.alpha) * ce_loss + self.alpha * custom_loss\n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = torch.tensor(training_X, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(train_data_y_discrete, dtype=torch.long).to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 2000\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the selected device\n",
    "input_size = 14\n",
    "model = MLPClassifierPyTorch(input_size).to(device)\n",
    "\n",
    "criterion = CombinedLoss(alpha=0.5,beta=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  27.41, \n",
      " 21.67,  24.16,  22.40,  22.10,  22.33,  22.85,  19.55,  26.41,  25.13, 10  19.03, \n",
      " 19.54,  23.34,  25.63,  24.36,  22.59,  20.83,  23.11,  28.18,  22.85, 20  18.01, \n",
      " 28.69,  23.88,  22.09,  25.90,  21.55,  24.87,  22.83,  23.35,  26.16, 30  25.38, \n",
      " 24.13,  25.88,  25.90,  27.16,  20.57,  23.60,  24.62,  26.42,  23.60, 40  23.36, \n",
      " 23.12,  22.84,  23.33,  23.10,  28.18,  22.31,  20.57,  23.61,  25.65, CPU times: total: 38.9 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training loop\n",
    "loss_history = []\n",
    "for epoch in range(50):\n",
    "    if epoch%10==0:\n",
    "        print(epoch, end=\" \")\n",
    "    avg_loss = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        avg_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_history.append(np.mean(avg_loss))\n",
    "    print(f\" {loss.item():.2f},\", end=\" \")\n",
    "    if epoch%10==0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Precision: 0.80, Recall: 1.00\n",
      "Class 1 - Precision: 0.00, Recall: 0.00\n",
      "Class 2 - Precision: 0.00, Recall: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate precision and recall for each class\n",
    "precision = precision_score(all_labels, all_predictions, average=None,zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average=None,zero_division=0)\n",
    "\n",
    "# Print precision and recall for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f'Class {i} - Precision: {precision[i]:.2f}, Recall: {recall[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
